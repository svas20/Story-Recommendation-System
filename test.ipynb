{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f4b59-a10b-4b7f-ba6b-1f80d3ce6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Optional\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "class ConfigSchema(TypedDict):\n",
    "    model: str\n",
    "    thread_id:str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5e4c2-e711-42f2-a0c0-2fd3007a4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def db_generator():\n",
    "    response = \"Best Story: Mr Bean Story\"\n",
    "    #text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    #   chunk_size=100, chunk_overlap=50\n",
    "    #   )\n",
    "    #txt_splits = text_splitter.split_text(response)\n",
    "    document = Document(\n",
    "    page_content=response,\n",
    "    metadata={\"source\": \"Feedback\"},\n",
    "    )\n",
    "    vectorstore = FAISS.from_documents([document], embedding=OpenAIEmbeddings())\n",
    "    vectorstore.save_local(\"feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f05e68-6a0f-49b7-8e9e-0dc15c935176",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afca11c-59cb-47d5-bb2d-9c7a444cca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import State\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from retriver import get_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "load_dotenv()\n",
    "\n",
    "def story_generator(state:State,config):\n",
    "    GENERATOR_PROMPTS =\"\"\" Write a short bedtime story, using simple vocabulary that is suitable for children aged 5 to 10.\n",
    "                        - Make the vocabulary more age-appropriate\n",
    "                        - Ensure the story is gentle, imaginative, and calming\n",
    "                        - Include a simple moral\n",
    "                        - Consider past feedback to guide your edits.\"\"\"\n",
    "    \n",
    "    \n",
    "    model = init_chat_model(model=str(config[\"configurable\"][\"model\"]),temperature = 0.1)\n",
    "    retriever_tool = get_retriever_tool()\n",
    "    story_response = model.invoke(\n",
    "    [{\"role\":\"system\",\"content\":GENERATOR_PROMPTS}],\n",
    "    state[\"messages\"]).bind_tools([retriever_tool], tool_choice= \"any\")\n",
    "    \n",
    "    #print(\"generator\",state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [story_response]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c15f1-b5c4-4953-bc2c-c56b2b6042c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "def get_retriever_tool():\n",
    "    vectorstore = FAISS.load_local(\n",
    "        \"feedback\",\n",
    "        embeddings=OpenAIEmbeddings(),\n",
    "        allow_dangerous_deserialization = True\n",
    "        \n",
    "    )\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    print(retriever.invoke(\"some comedy sotry\"))\n",
    "    return create_retriever_tool(\n",
    "        retriever,\n",
    "        name=\"feedback\",\n",
    "        description=\"Use feedback from previous stories to guide improvements.\"\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcd56e-4752-4da0-88e8-33bd6b4548b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = get_retriever_tool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6ad06-90c0-4d1c-894d-bd290eba9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool.invoke(\"comedy story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51877b60-5fc0-4dc7-9641-ad35057a2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool.invoke({\"query\": \"some comedy story\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e5ddd-fbd4-41e1-b70d-aa6751f0f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = ToolNode([retriever_tool])\n",
    "print(tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008eaf5-2412-4c35-ba32-195bd7c27619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages.tool import ToolMessage\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "retriever_tool = get_retriever_tool()\n",
    "\n",
    "tool_node = ToolNode(tools=[retriever_tool])\n",
    "\n",
    "tool_call_id = f\"call_{uuid4().hex}\" \n",
    "ai_message = AIMessage(\n",
    "    content=\"\",  \n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"name\": \"feedback\",\n",
    "            \"args\": {\"query\": \"can i listen to the story of an old man\"},\n",
    "            \"id\": tool_call_id,\n",
    "            \"type\": \"function\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_state = {\"messages\": [ai_message]}\n",
    "\n",
    "output = tool_node.invoke(input_state)\n",
    "\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cbee3-6f47-4453-9605-fd907a63b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import State\n",
    "from dotenv import load_dotenv\n",
    "from feedback import feedback\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.prebuilt.interrupt import HumanInterrupt\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "def LLMJudge(state:State):\n",
    "    request: HumanInterrupt = {\n",
    "        \"action_request\":{\n",
    "                \"action\": \"Feedback\",\n",
    "                \"args\": state[\"messages\"]\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"allow_ignore\": True,\n",
    "            \"allow_respond\": True,\n",
    "            \"allow_edit\": False,\n",
    "            \"allow_accept\": False\n",
    "        },\n",
    "        \"description\": \"Please provide the Feedback\"\n",
    "    }\n",
    "     \n",
    "    response = interrupt([request])\n",
    "    print(response)\n",
    "    if response[0][\"type\"] == \"response\":\n",
    "       return Command(goto=[\"feedback\"],update=\"Feedback: \"+response[0][\"args\"])\n",
    "    if response[0][\"type\"] == \"ignore\":\n",
    "        return Command(goto=[\"feedback\"],update=\"Feedback: \"+response[0][\"args\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776710dc-951c-4b21-afbe-27a1de762a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import State\n",
    "from typing import Optional\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def feedback(state:State,config):\n",
    "    FEEDBACK_PROMPTS =\"\"\"You are a feedback summarizer for a bedtime storytelling assistant. Your role is to review the entire conversation and generate a brief summary in 3â€“4 sentences.\n",
    "                            Instructions:\n",
    "                            - If the user provided explicit feedback, identify which part of the story they referred to and summarize their likes or dislikes accordingly.                            \n",
    "                            - If no feedback was provided, summarize the storyline requested and the story generated\n",
    "                        \"\"\"\n",
    "    \n",
    "   \n",
    "    model = init_chat_model(model=str(config[\"configurable\"][\"model\"]),temperature = 0.1)\n",
    "    Judge_response = model.invoke([\n",
    "        {\"role\": \"system\", \"content\": FEEDBACK_PROMPTS},\n",
    "         state[\"messages\"],\n",
    "    ])\n",
    "    #text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    #    chunk_size=100, chunk_overlap=50\n",
    "    #    )\n",
    "    #txt_splits = text_splitter.split_text(Judge_response.content)\n",
    "    document = Document(\n",
    "    page_content=Judge_response,\n",
    "    metadata={\"source\": \"Feedback\"},\n",
    "    )\n",
    "    vectorstore = FAISS.add_documents([document], embedding=OpenAIEmbeddings())\n",
    "    vectorstore.save_local(\"feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1347c945-b8de-4201-9477-ea548c4e0a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1:\n",
      "The user is interested in fairy tales, moral stories, and animal stories.\n",
      "---\n",
      "Doc 2:\n",
      "On 06/02/2025, no specific feedback was provided for the bedtime story generated. The story featured a curious squirrel named Lily who embarked on a magical adventure in the forest, discovering the true treasures of kindness and love.\n",
      "---\n",
      "Doc 3:\n",
      "Based on the feedback provided on 06/02/2025, it is important to avoid repeating the character \"Sammy\" and the same storyline in future bedtime stories. Users have expressed a desire for variety and fresh narratives in the stories generated.\n",
      "---\n",
      "Doc 4:\n",
      "The user provided feedback on 06/02/2025 requesting to avoid repeating the story and characters in future bedtime stories.\n",
      "---\n",
      "Doc 5:\n",
      "Feedback received on 06/02/2025: The user liked the story as characters were not repeated.\n",
      "\n",
      "Based on the feedback, the user appreciated the fresh characters in the story and enjoyed the narrative without character repetition.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "load_dotenv()\n",
    "vectorstore = FAISS.load_local(\"feedback\", OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "all_docs = vectorstore.similarity_search(query=\"\", k=5)\n",
    "\n",
    "for i, doc in enumerate(all_docs):\n",
    "    print(f\"Doc {i+1}:\\n{doc.page_content}\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10e1bcc9-77e6-47fa-8676-a051175c2231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'feedback' from 'feedback' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InMemorySaver\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLLMJudge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMJudge\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfeedback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feedback\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstory_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m story_generator\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'feedback' from 'feedback' (unknown location)"
     ]
    }
   ],
   "source": [
    "\n",
    "from config import State, ConfigSchema\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from LLMJudge import LLMJudge\n",
    "from feedback import feedback\n",
    "from story_generator import story_generator\n",
    "from pathlib import Path\n",
    "from retriver import get_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import convert_to_messages\n",
    "from db_generate import db_generator\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "load_dotenv()\n",
    "feedback_path = Path(\"feedback\")\n",
    "if not feedback_path.exists(): \n",
    "    db_generator()                      \n",
    "\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if not last_message.tool_calls:\n",
    "        return \"LLMJudge\"\n",
    "    else:\n",
    "        return \"retrieve\"\n",
    "retriever_tool = get_retriever_tool()\n",
    "builder = StateGraph(State, config_schema=ConfigSchema)\n",
    "config = {\"configurable\": {\"model\":  os.getenv(\"MODEL\"), \"thread_id\":\"story_1\"}}\n",
    "builder.add_node(\"story_generator\", story_generator)\n",
    "builder.add_node(\"retrieve\",ToolNode([retriever_tool]))\n",
    "builder.add_node(\"LLMJudge\",LLMJudge)\n",
    "builder.add_node(\"feedback\", feedback)\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"story_generator\")\n",
    "builder.add_conditional_edges(\"story_generator\",should_continue, {\"retrieve\":\"retrieve\",\"LLMJudge\":\"LLMJudge\"})\n",
    "builder.add_edge(\"retrieve\", \"story_generator\")\n",
    "builder.add_edge(\"LLMJudge\",\"feedback\")\n",
    "builder.add_edge(\"feedback\",END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "graph_dia = graph.get_graph().draw_mermaid_png()\n",
    "\n",
    "with open(\"mermaid_diagram.png\", \"wb\") as f:\n",
    "    f.write(graph_dia)\n",
    "\n",
    "\n",
    "input_state = {\"messages\": [HumanMessage(content=\"I want to listen to some best story\")]}\n",
    "result = graph.invoke(input_state,config=config)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d04270-7cc2-497f-afba-fc457b11d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, AnyMessage\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from typing import Annotated\n",
    "from langchain.chat_models import init_chat_model\n",
    "load_dotenv()\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Optional\n",
    "\n",
    "from langchain_core.messages import BaseMessage \n",
    "class State(MessagesState):\n",
    "    messages: list[BaseMessage]\n",
    "\n",
    "class ConfigSchema(TypedDict):\n",
    "    model: str\n",
    "    thread_id:str\n",
    "\n",
    "# --- Step 3: Create ToolNode ---\n",
    "retriever_tool = get_retriever_tool()\n",
    "tool_node = ToolNode(\n",
    "    tools=[retriever_tool],\n",
    "    name=\"tools\",\n",
    "    handle_tool_errors=True\n",
    ")\n",
    "\n",
    "# --- Step 4: LLM node that generates tool_calls ---\n",
    "def llm_node(state: State) -> State:\n",
    "    model = init_chat_model(model=str(config[\"configurable\"][\"model\"]),temperature = 0.1)\n",
    "    model_with_tools = model.bind_tools([retriever_tool], tool_choice= \"any\")\n",
    "    response=model_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "# --- Step 5: Assemble the graph ---\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"llm\", llm_node)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph.set_entry_point(\"llm\")\n",
    "graph.add_edge(\"llm\", \"tools\")\n",
    "graph.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "# --- Step 7: Run the graph with an input message ---\n",
    "input_state = {\n",
    "    \"messages\": [HumanMessage(content=\"some comedy story\")]\n",
    "}\n",
    "\n",
    "result = app.invoke(input_state,config=config)\n",
    "\n",
    "# --- Step 8: Output the result ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(result[\"messages\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f8aedc-9fb1-4b8c-b7c2-0b1bc57412e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/02/2025\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "today_str = datetime.today().strftime(\"%m/%d/%Y\") \n",
    "print(today_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2365c6-7d7e-4b82-a923-46ff379d3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
